# Chapter 4: Numerical Computation

**Underflow**: Occurs when numbers near zero are rounded to zero.

**Overflow**: Occurs when numbers with large magnitude are approximated as ∞ or -∞.

**Condition number**: The ratio of the magnitude of the largest and smallest eigenvalue.

**Optimization**: The task of either minimizing or maximizing some function f(x) by altering x. We usually phrase most optimization problems in terms of minimizing f(x).

**Objective function**: The function we want to minimize or maximize. Also known as the criterion. When we are minimizing it, we may also call it the cost function, loss function, or error function. We often denote the value that minimizes or maximizes a function with a superscript \*. For example, we might say x^\*^ = arg min f(x).

**Gradient descent**: Reducing f(x) by moving x in small steps with the opposite sign of the derivative.

**Directional derivative**: The slope of the function f in direction u (a unit vector).

**Jacobian matrix**: The matrix containing all partial derivatives of a function whose input and output are both vectors.

**Hessian matrix**: The matrix of second-order partial derivatives for a function with multiple input dimensions. Can be used to determine the curvature of a function.

**Constrained optimization**: Finding the maximal or minimal value of f(x) for values of x in some set S. Points *x* that lie within the set S are called feasible points in constrained optimization terminology.
