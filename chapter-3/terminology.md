# Chapter 3: Probability and Information Theory

**Random variable**: A variable that can take on different values randomly. On its own, a random variable is just a description of the states that are possible; it must be coupled with a probability distribution that specifies how likely each of these states are.

**Probability distribution**: A description of how likely a random variable or set of random variables os to take on each of its possible states.

**Probability mass function (PMF)**: A probability distribution over discrete random variables. Maps from a state of a random variable to the probability of that random variable taking on that state. If P(_x_) has a probability of 1, x=_x_ is certain.

**Joint probability distribution**: A probability distribution over many variables. P(x=_x_, y=_y_) denotes the probability that x=_x_ and y=_y_ simultaneously.

**Probability density function (PDF)**: A probability distribution over continuous random variables. Often denoted by _p_.

**Marginal probability**: The probability distribution over a subset of variables for which we know the probability distribution.

**Conditional probability**: The probability of some event given that some other event has happened. We denote the conditional probability that y=_y_ given x=_x_ as P(y=_y_|x=_x_).

**Independence**: Two random variables x and y are independent if their probability distribution can be expressed as a product of two factors, one involving only x and one involving only y.

**Latent variable**: A random variable that we cannot observe directly.

**Information theory**: A branch of applied mathematics that revolves around quantifying how much information is present in a signal.
